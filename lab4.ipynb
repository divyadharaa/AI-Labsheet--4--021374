{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPI8Y7V+yOiZjkkRaTEy6Jc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/divyadharaa/AI-Labsheet--4--021374/blob/main/lab4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "36BOPxh5BNZ5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**AI LABSHEET: AGENTS AND KNN**\n",
        "\n",
        "\n",
        "**Name**: Salina Kunwar\n",
        "**Roll no**: 021-374\n"
      ],
      "metadata": {
        "id": "d0PdBNm3BfCy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perceptron for 2-Input Basic Gates (AND/OR)\n"
      ],
      "metadata": {
        "id": "WW7NYAzFB4FY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def linearInt(x, weights, bias):\n",
        "    return sum([x[i] * weights[i] for i in range(len(x))]) + bias\n",
        "\n",
        "def predict(x, weights, bias):\n",
        "    return 1 if linearInt(x, weights, bias) >= 0 else 0\n",
        "\n",
        "def train_perceptron(X, Y, learning_rate=0.1, max_iterations=100):\n",
        "    weights = [0, 0]\n",
        "    bias = 0\n",
        "\n",
        "    for _ in range(max_iterations):  # max_iterations to avoid infinite loop\n",
        "        error_found = False\n",
        "        for i in range(len(X)):\n",
        "            x = X[i]\n",
        "            y = Y[i]\n",
        "            y_pred = predict(x, weights, bias)\n",
        "            error = y - y_pred\n",
        "\n",
        "            if error != 0:\n",
        "                # Update weights and bias\n",
        "                for j in range(len(weights)):\n",
        "                    weights[j] += learning_rate * error * x[j]\n",
        "                bias += learning_rate * error\n",
        "                error_found = True\n",
        "\n",
        "        if not error_found:\n",
        "            break  # Stop if no errors (converged)\n",
        "\n",
        "    return weights, bias\n",
        "\n",
        "def accuracy(X, Y, weights, bias):\n",
        "    correct = 0\n",
        "    for i in range(len(X)):\n",
        "        if predict(X[i], weights, bias) == Y[i]:\n",
        "            correct += 1\n",
        "    return (correct / len(X)) * 100\n",
        "\n",
        "# Input truth table\n",
        "X = [[0, 0], [0, 1], [1, 0], [1, 1]]\n",
        "\n",
        "# AND Gate\n",
        "print(\"---- AND Gate ----\")\n",
        "Y_and = [0, 0, 0, 1]\n",
        "w_and, b_and = train_perceptron(X, Y_and)\n",
        "print(f\"Weights: {w_and}, Bias: {b_and}\")\n",
        "print(f\"Accuracy: {accuracy(X, Y_and, w_and, b_and)}%\")\n",
        "\n",
        "# OR Gate\n",
        "print(\"\\n---- OR Gate ----\")\n",
        "Y_or = [0, 1, 1, 1]\n",
        "w_or, b_or = train_perceptron(X, Y_or)\n",
        "print(f\"Weights: {w_or}, Bias: {b_or}\")\n",
        "print(f\"Accuracy: {accuracy(X, Y_or, w_or, b_or)}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xlu8_FfBr0x",
        "outputId": "4100b203-379c-4f6d-d5b3-a78a243edfdc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---- AND Gate ----\n",
            "Weights: [0.2, 0.1], Bias: -0.20000000000000004\n",
            "Accuracy: 100.0%\n",
            "\n",
            "---- OR Gate ----\n",
            "Weights: [0.1, 0.1], Bias: -0.1\n",
            "Accuracy: 100.0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perceptron for n-Input Basic Gates (AND/OR)\n"
      ],
      "metadata": {
        "id": "4ZE1hHNRCDbt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import product\n",
        "\n",
        "def linearInt(x, weights, bias):\n",
        "    return sum([x[i] * weights[i] for i in range(len(x))]) + bias\n",
        "\n",
        "def predict(x, weights, bias):\n",
        "    return 1 if linearInt(x, weights, bias) >= 0 else 0\n",
        "\n",
        "def train(X, Y, lr=0.1, max_loops=100):\n",
        "    weights = [0.0] * len(X[0])\n",
        "    bias = 0.0\n",
        "\n",
        "    for _ in range(max_loops):\n",
        "        error_found = False\n",
        "        for i in range(len(X)):\n",
        "            y_pred = predict(X[i], weights, bias)\n",
        "            error = Y[i] - y_pred\n",
        "            if error != 0:\n",
        "                for j in range(len(weights)):\n",
        "                    weights[j] += lr * error * X[i][j]\n",
        "                bias += lr * error\n",
        "                error_found = True\n",
        "        if not error_found:\n",
        "            break\n",
        "\n",
        "    return weights, bias\n",
        "\n",
        "def test_accuracy(X, Y, weights, bias):\n",
        "    correct = 0\n",
        "    for i in range(len(X)):\n",
        "        if predict(X[i], weights, bias) == Y[i]:\n",
        "            correct += 1\n",
        "    return (correct / len(X)) * 100\n",
        "\n",
        "# Run for both 3-input and 4-input gates\n",
        "for n in [3, 4]:\n",
        "    print(f\"\\n==== {n}-INPUT GATES ====\")\n",
        "    X = list(product([0, 1], repeat=n))\n",
        "\n",
        "    # AND Gate\n",
        "    Y_and = [int(all(x)) for x in X]\n",
        "    w_and, b_and = train(X, Y_and)\n",
        "    print(f\"\\nAND Gate:\")\n",
        "    print(f\"Weights: {w_and}\")\n",
        "    print(f\"Bias: {b_and}\")\n",
        "    print(f\"Accuracy: {test_accuracy(X, Y_and, w_and, b_and)}%\")\n",
        "\n",
        "    # OR Gate\n",
        "    Y_or = [int(any(x)) for x in X]\n",
        "    w_or, b_or = train(X, Y_or)\n",
        "    print(f\"\\nOR Gate:\")\n",
        "    print(f\"Weights: {w_or}\")\n",
        "    print(f\"Bias: {b_or}\")\n",
        "    print(f\"Accuracy: {test_accuracy(X, Y_or, w_or, b_or)}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gPLCv-wUCEOA",
        "outputId": "7d13dbd0-7b8e-4618-d970-a4881d36cbc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==== 3-INPUT GATES ====\n",
            "\n",
            "AND Gate:\n",
            "Weights: [0.1, 0.1, 0.1]\n",
            "Bias: -0.20000000000000004\n",
            "Accuracy: 100.0%\n",
            "\n",
            "OR Gate:\n",
            "Weights: [0.1, 0.1, 0.1]\n",
            "Bias: -0.1\n",
            "Accuracy: 100.0%\n",
            "\n",
            "==== 4-INPUT GATES ====\n",
            "\n",
            "AND Gate:\n",
            "Weights: [0.4, 0.20000000000000004, 0.1, 0.1]\n",
            "Bias: -0.7999999999999999\n",
            "Accuracy: 100.0%\n",
            "\n",
            "OR Gate:\n",
            "Weights: [0.1, 0.1, 0.1, 0.1]\n",
            "Bias: -0.1\n",
            "Accuracy: 100.0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perceptron for Linear Function with 3 Features\n"
      ],
      "metadata": {
        "id": "ZAcXO8PUCKAj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# Generate dataset: y = 2x1 + 3x2 - x3 + 5\n",
        "X = []\n",
        "Y = []\n",
        "for _ in range(10):\n",
        "    x1 = random.uniform(0, 1)\n",
        "    x2 = random.uniform(0, 1)\n",
        "    x3 = random.uniform(0, 1)\n",
        "    y = 2 * x1 + 3 * x2 - 1 * x3 + 5\n",
        "    X.append([x1, x2, x3])\n",
        "    Y.append(y)\n",
        "\n",
        "# Initialize weights and bias\n",
        "weights = [0.0, 0.0, 0.0]\n",
        "bias = 0.0\n",
        "lr = 0.01\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(100):\n",
        "    total_error = 0\n",
        "    for i in range(len(X)):\n",
        "        x = X[i]\n",
        "        y_true = Y[i]\n",
        "\n",
        "        # Linear output (no activation)\n",
        "        y_pred = sum([x[j] * weights[j] for j in range(3)]) + bias\n",
        "\n",
        "        # Error\n",
        "        error = y_true - y_pred\n",
        "\n",
        "        # Update weights and bias\n",
        "        for j in range(3):\n",
        "            weights[j] += lr * error * x[j]\n",
        "        bias += lr * error\n",
        "\n",
        "        total_error += error ** 2  # squared error\n",
        "\n",
        "    mse = total_error / len(X)\n",
        "    print(f\"Epoch {epoch+1}: MSE = {mse:.4f}\")\n",
        "\n",
        "# Final result\n",
        "print(\"\\nFinal Weights and Bias:\")\n",
        "print(f\"Weights: {weights}\")\n",
        "print(f\"Bias: {bias}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JyMsYlZjCK1Y",
        "outputId": "38fce5e3-a1e1-4932-97fa-44ea217af3f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: MSE = 42.6729\n",
            "Epoch 2: MSE = 29.9478\n",
            "Epoch 3: MSE = 21.1186\n",
            "Epoch 4: MSE = 14.9893\n",
            "Epoch 5: MSE = 10.7314\n",
            "Epoch 6: MSE = 7.7709\n",
            "Epoch 7: MSE = 5.7098\n",
            "Epoch 8: MSE = 4.2726\n",
            "Epoch 9: MSE = 3.2682\n",
            "Epoch 10: MSE = 2.5642\n",
            "Epoch 11: MSE = 2.0687\n",
            "Epoch 12: MSE = 1.7181\n",
            "Epoch 13: MSE = 1.4682\n",
            "Epoch 14: MSE = 1.2885\n",
            "Epoch 15: MSE = 1.1576\n",
            "Epoch 16: MSE = 1.0608\n",
            "Epoch 17: MSE = 0.9879\n",
            "Epoch 18: MSE = 0.9317\n",
            "Epoch 19: MSE = 0.8873\n",
            "Epoch 20: MSE = 0.8513\n",
            "Epoch 21: MSE = 0.8211\n",
            "Epoch 22: MSE = 0.7953\n",
            "Epoch 23: MSE = 0.7725\n",
            "Epoch 24: MSE = 0.7520\n",
            "Epoch 25: MSE = 0.7332\n",
            "Epoch 26: MSE = 0.7157\n",
            "Epoch 27: MSE = 0.6993\n",
            "Epoch 28: MSE = 0.6836\n",
            "Epoch 29: MSE = 0.6685\n",
            "Epoch 30: MSE = 0.6541\n",
            "Epoch 31: MSE = 0.6401\n",
            "Epoch 32: MSE = 0.6265\n",
            "Epoch 33: MSE = 0.6133\n",
            "Epoch 34: MSE = 0.6005\n",
            "Epoch 35: MSE = 0.5880\n",
            "Epoch 36: MSE = 0.5758\n",
            "Epoch 37: MSE = 0.5638\n",
            "Epoch 38: MSE = 0.5522\n",
            "Epoch 39: MSE = 0.5408\n",
            "Epoch 40: MSE = 0.5297\n",
            "Epoch 41: MSE = 0.5188\n",
            "Epoch 42: MSE = 0.5082\n",
            "Epoch 43: MSE = 0.4978\n",
            "Epoch 44: MSE = 0.4876\n",
            "Epoch 45: MSE = 0.4777\n",
            "Epoch 46: MSE = 0.4679\n",
            "Epoch 47: MSE = 0.4584\n",
            "Epoch 48: MSE = 0.4491\n",
            "Epoch 49: MSE = 0.4400\n",
            "Epoch 50: MSE = 0.4311\n",
            "Epoch 51: MSE = 0.4223\n",
            "Epoch 52: MSE = 0.4138\n",
            "Epoch 53: MSE = 0.4054\n",
            "Epoch 54: MSE = 0.3972\n",
            "Epoch 55: MSE = 0.3892\n",
            "Epoch 56: MSE = 0.3814\n",
            "Epoch 57: MSE = 0.3737\n",
            "Epoch 58: MSE = 0.3662\n",
            "Epoch 59: MSE = 0.3589\n",
            "Epoch 60: MSE = 0.3517\n",
            "Epoch 61: MSE = 0.3447\n",
            "Epoch 62: MSE = 0.3378\n",
            "Epoch 63: MSE = 0.3311\n",
            "Epoch 64: MSE = 0.3245\n",
            "Epoch 65: MSE = 0.3180\n",
            "Epoch 66: MSE = 0.3117\n",
            "Epoch 67: MSE = 0.3055\n",
            "Epoch 68: MSE = 0.2995\n",
            "Epoch 69: MSE = 0.2935\n",
            "Epoch 70: MSE = 0.2877\n",
            "Epoch 71: MSE = 0.2821\n",
            "Epoch 72: MSE = 0.2765\n",
            "Epoch 73: MSE = 0.2711\n",
            "Epoch 74: MSE = 0.2657\n",
            "Epoch 75: MSE = 0.2605\n",
            "Epoch 76: MSE = 0.2554\n",
            "Epoch 77: MSE = 0.2504\n",
            "Epoch 78: MSE = 0.2456\n",
            "Epoch 79: MSE = 0.2408\n",
            "Epoch 80: MSE = 0.2361\n",
            "Epoch 81: MSE = 0.2315\n",
            "Epoch 82: MSE = 0.2270\n",
            "Epoch 83: MSE = 0.2226\n",
            "Epoch 84: MSE = 0.2183\n",
            "Epoch 85: MSE = 0.2141\n",
            "Epoch 86: MSE = 0.2100\n",
            "Epoch 87: MSE = 0.2059\n",
            "Epoch 88: MSE = 0.2020\n",
            "Epoch 89: MSE = 0.1981\n",
            "Epoch 90: MSE = 0.1943\n",
            "Epoch 91: MSE = 0.1906\n",
            "Epoch 92: MSE = 0.1870\n",
            "Epoch 93: MSE = 0.1834\n",
            "Epoch 94: MSE = 0.1799\n",
            "Epoch 95: MSE = 0.1765\n",
            "Epoch 96: MSE = 0.1732\n",
            "Epoch 97: MSE = 0.1699\n",
            "Epoch 98: MSE = 0.1667\n",
            "Epoch 99: MSE = 0.1635\n",
            "Epoch 100: MSE = 0.1605\n",
            "\n",
            "Final Weights and Bias:\n",
            "Weights: [2.1189537554512934, 2.960675906459197, 0.21194516528946697]\n",
            "Bias: 4.265976508014334\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perceptron for Linear Function with n Features\n"
      ],
      "metadata": {
        "id": "ZOnjqrj3CQZY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def train_perceptron_linear(X, Y, lr=0.01, epochs=100):\n",
        "    n_features = len(X[0])\n",
        "    weights = [0.0] * n_features\n",
        "    bias = 0.0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_error = 0\n",
        "        for i in range(len(X)):\n",
        "            y_pred = sum(X[i][j] * weights[j] for j in range(n_features)) + bias\n",
        "            error = Y[i] - y_pred\n",
        "            # Update weights and bias\n",
        "            for j in range(n_features):\n",
        "                weights[j] += lr * error * X[i][j]\n",
        "            bias += lr * error\n",
        "            total_error += error ** 2\n",
        "        mse = total_error / len(X)\n",
        "        print(f\"Epoch {epoch+1}: MSE = {mse:.4f}\")\n",
        "\n",
        "    return weights, bias\n",
        "\n",
        "\n",
        "def generate_dataset(n_features, n_samples=10):\n",
        "    # Generate true random weights in [-1,1]\n",
        "    true_weights = [random.uniform(-1, 1) for _ in range(n_features)]\n",
        "    true_bias = 5  # fixed bias\n",
        "\n",
        "    X = []\n",
        "    Y = []\n",
        "    for _ in range(n_samples):\n",
        "        features = [random.uniform(0, 1) for _ in range(n_features)]\n",
        "        y = sum(features[i] * true_weights[i] for i in range(n_features)) + true_bias\n",
        "        X.append(features)\n",
        "        Y.append(y)\n",
        "\n",
        "    return X, Y, true_weights, true_bias\n",
        "\n",
        "\n",
        "# Test for n=4 and n=5\n",
        "for n in [4, 5]:\n",
        "    print(f\"\\nTraining for n={n} features:\")\n",
        "    X, Y, true_w, true_b = generate_dataset(n)\n",
        "    print(f\"True weights: {true_w}\")\n",
        "    print(f\"True bias: {true_b}\")\n",
        "    learned_weights, learned_bias = train_perceptron_linear(X, Y)\n",
        "    print(f\"Learned weights: {learned_weights}\")\n",
        "    print(f\"Learned bias: {learned_bias}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_WuvGlBtCROg",
        "outputId": "a8ce1145-836f-45dc-9d20-a8d5b9a7be39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training for n=4 features:\n",
            "True weights: [-0.79603054686971, 0.8968228432246508, -0.2743821663270265, 0.05821653185084452]\n",
            "True bias: 5\n",
            "Epoch 1: MSE = 19.7075\n",
            "Epoch 2: MSE = 13.4176\n",
            "Epoch 3: MSE = 9.1858\n",
            "Epoch 4: MSE = 6.3380\n",
            "Epoch 5: MSE = 4.4210\n",
            "Epoch 6: MSE = 3.1301\n",
            "Epoch 7: MSE = 2.2601\n",
            "Epoch 8: MSE = 1.6732\n",
            "Epoch 9: MSE = 1.2767\n",
            "Epoch 10: MSE = 1.0082\n",
            "Epoch 11: MSE = 0.8259\n",
            "Epoch 12: MSE = 0.7015\n",
            "Epoch 13: MSE = 0.6160\n",
            "Epoch 14: MSE = 0.5568\n",
            "Epoch 15: MSE = 0.5152\n",
            "Epoch 16: MSE = 0.4856\n",
            "Epoch 17: MSE = 0.4640\n",
            "Epoch 18: MSE = 0.4477\n",
            "Epoch 19: MSE = 0.4352\n",
            "Epoch 20: MSE = 0.4252\n",
            "Epoch 21: MSE = 0.4169\n",
            "Epoch 22: MSE = 0.4097\n",
            "Epoch 23: MSE = 0.4034\n",
            "Epoch 24: MSE = 0.3977\n",
            "Epoch 25: MSE = 0.3923\n",
            "Epoch 26: MSE = 0.3873\n",
            "Epoch 27: MSE = 0.3825\n",
            "Epoch 28: MSE = 0.3779\n",
            "Epoch 29: MSE = 0.3735\n",
            "Epoch 30: MSE = 0.3691\n",
            "Epoch 31: MSE = 0.3649\n",
            "Epoch 32: MSE = 0.3608\n",
            "Epoch 33: MSE = 0.3568\n",
            "Epoch 34: MSE = 0.3529\n",
            "Epoch 35: MSE = 0.3490\n",
            "Epoch 36: MSE = 0.3452\n",
            "Epoch 37: MSE = 0.3415\n",
            "Epoch 38: MSE = 0.3378\n",
            "Epoch 39: MSE = 0.3343\n",
            "Epoch 40: MSE = 0.3307\n",
            "Epoch 41: MSE = 0.3273\n",
            "Epoch 42: MSE = 0.3239\n",
            "Epoch 43: MSE = 0.3206\n",
            "Epoch 44: MSE = 0.3173\n",
            "Epoch 45: MSE = 0.3141\n",
            "Epoch 46: MSE = 0.3110\n",
            "Epoch 47: MSE = 0.3079\n",
            "Epoch 48: MSE = 0.3048\n",
            "Epoch 49: MSE = 0.3018\n",
            "Epoch 50: MSE = 0.2989\n",
            "Epoch 51: MSE = 0.2960\n",
            "Epoch 52: MSE = 0.2931\n",
            "Epoch 53: MSE = 0.2903\n",
            "Epoch 54: MSE = 0.2876\n",
            "Epoch 55: MSE = 0.2849\n",
            "Epoch 56: MSE = 0.2822\n",
            "Epoch 57: MSE = 0.2796\n",
            "Epoch 58: MSE = 0.2770\n",
            "Epoch 59: MSE = 0.2745\n",
            "Epoch 60: MSE = 0.2720\n",
            "Epoch 61: MSE = 0.2695\n",
            "Epoch 62: MSE = 0.2671\n",
            "Epoch 63: MSE = 0.2647\n",
            "Epoch 64: MSE = 0.2624\n",
            "Epoch 65: MSE = 0.2601\n",
            "Epoch 66: MSE = 0.2578\n",
            "Epoch 67: MSE = 0.2555\n",
            "Epoch 68: MSE = 0.2533\n",
            "Epoch 69: MSE = 0.2512\n",
            "Epoch 70: MSE = 0.2490\n",
            "Epoch 71: MSE = 0.2469\n",
            "Epoch 72: MSE = 0.2448\n",
            "Epoch 73: MSE = 0.2428\n",
            "Epoch 74: MSE = 0.2407\n",
            "Epoch 75: MSE = 0.2388\n",
            "Epoch 76: MSE = 0.2368\n",
            "Epoch 77: MSE = 0.2348\n",
            "Epoch 78: MSE = 0.2329\n",
            "Epoch 79: MSE = 0.2311\n",
            "Epoch 80: MSE = 0.2292\n",
            "Epoch 81: MSE = 0.2274\n",
            "Epoch 82: MSE = 0.2256\n",
            "Epoch 83: MSE = 0.2238\n",
            "Epoch 84: MSE = 0.2220\n",
            "Epoch 85: MSE = 0.2203\n",
            "Epoch 86: MSE = 0.2186\n",
            "Epoch 87: MSE = 0.2169\n",
            "Epoch 88: MSE = 0.2152\n",
            "Epoch 89: MSE = 0.2135\n",
            "Epoch 90: MSE = 0.2119\n",
            "Epoch 91: MSE = 0.2103\n",
            "Epoch 92: MSE = 0.2087\n",
            "Epoch 93: MSE = 0.2072\n",
            "Epoch 94: MSE = 0.2056\n",
            "Epoch 95: MSE = 0.2041\n",
            "Epoch 96: MSE = 0.2026\n",
            "Epoch 97: MSE = 0.2011\n",
            "Epoch 98: MSE = 0.1996\n",
            "Epoch 99: MSE = 0.1982\n",
            "Epoch 100: MSE = 0.1967\n",
            "Learned weights: [0.41562524526830846, 1.4694429818789803, 1.0283695647482365, 1.0160210515576333]\n",
            "Learned bias: 2.9458627543544686\n",
            "\n",
            "Training for n=5 features:\n",
            "True weights: [-0.14586406591550705, 0.7987403375357387, 0.9846513742829524, -0.7604306672865626, 0.534574971773913]\n",
            "True bias: 5\n",
            "Epoch 1: MSE = 26.5324\n",
            "Epoch 2: MSE = 16.4937\n",
            "Epoch 3: MSE = 10.3261\n",
            "Epoch 4: MSE = 6.5342\n",
            "Epoch 5: MSE = 4.2006\n",
            "Epoch 6: MSE = 2.7626\n",
            "Epoch 7: MSE = 1.8747\n",
            "Epoch 8: MSE = 1.3251\n",
            "Epoch 9: MSE = 0.9836\n",
            "Epoch 10: MSE = 0.7703\n",
            "Epoch 11: MSE = 0.6360\n",
            "Epoch 12: MSE = 0.5506\n",
            "Epoch 13: MSE = 0.4954\n",
            "Epoch 14: MSE = 0.4590\n",
            "Epoch 15: MSE = 0.4342\n",
            "Epoch 16: MSE = 0.4168\n",
            "Epoch 17: MSE = 0.4041\n",
            "Epoch 18: MSE = 0.3942\n",
            "Epoch 19: MSE = 0.3863\n",
            "Epoch 20: MSE = 0.3796\n",
            "Epoch 21: MSE = 0.3737\n",
            "Epoch 22: MSE = 0.3684\n",
            "Epoch 23: MSE = 0.3634\n",
            "Epoch 24: MSE = 0.3588\n",
            "Epoch 25: MSE = 0.3543\n",
            "Epoch 26: MSE = 0.3501\n",
            "Epoch 27: MSE = 0.3459\n",
            "Epoch 28: MSE = 0.3419\n",
            "Epoch 29: MSE = 0.3380\n",
            "Epoch 30: MSE = 0.3341\n",
            "Epoch 31: MSE = 0.3304\n",
            "Epoch 32: MSE = 0.3267\n",
            "Epoch 33: MSE = 0.3231\n",
            "Epoch 34: MSE = 0.3195\n",
            "Epoch 35: MSE = 0.3160\n",
            "Epoch 36: MSE = 0.3126\n",
            "Epoch 37: MSE = 0.3092\n",
            "Epoch 38: MSE = 0.3059\n",
            "Epoch 39: MSE = 0.3026\n",
            "Epoch 40: MSE = 0.2994\n",
            "Epoch 41: MSE = 0.2962\n",
            "Epoch 42: MSE = 0.2931\n",
            "Epoch 43: MSE = 0.2900\n",
            "Epoch 44: MSE = 0.2870\n",
            "Epoch 45: MSE = 0.2840\n",
            "Epoch 46: MSE = 0.2811\n",
            "Epoch 47: MSE = 0.2782\n",
            "Epoch 48: MSE = 0.2753\n",
            "Epoch 49: MSE = 0.2725\n",
            "Epoch 50: MSE = 0.2698\n",
            "Epoch 51: MSE = 0.2671\n",
            "Epoch 52: MSE = 0.2644\n",
            "Epoch 53: MSE = 0.2617\n",
            "Epoch 54: MSE = 0.2591\n",
            "Epoch 55: MSE = 0.2566\n",
            "Epoch 56: MSE = 0.2540\n",
            "Epoch 57: MSE = 0.2515\n",
            "Epoch 58: MSE = 0.2491\n",
            "Epoch 59: MSE = 0.2467\n",
            "Epoch 60: MSE = 0.2443\n",
            "Epoch 61: MSE = 0.2419\n",
            "Epoch 62: MSE = 0.2396\n",
            "Epoch 63: MSE = 0.2373\n",
            "Epoch 64: MSE = 0.2350\n",
            "Epoch 65: MSE = 0.2328\n",
            "Epoch 66: MSE = 0.2306\n",
            "Epoch 67: MSE = 0.2284\n",
            "Epoch 68: MSE = 0.2263\n",
            "Epoch 69: MSE = 0.2242\n",
            "Epoch 70: MSE = 0.2221\n",
            "Epoch 71: MSE = 0.2201\n",
            "Epoch 72: MSE = 0.2180\n",
            "Epoch 73: MSE = 0.2160\n",
            "Epoch 74: MSE = 0.2141\n",
            "Epoch 75: MSE = 0.2121\n",
            "Epoch 76: MSE = 0.2102\n",
            "Epoch 77: MSE = 0.2083\n",
            "Epoch 78: MSE = 0.2064\n",
            "Epoch 79: MSE = 0.2046\n",
            "Epoch 80: MSE = 0.2028\n",
            "Epoch 81: MSE = 0.2010\n",
            "Epoch 82: MSE = 0.1992\n",
            "Epoch 83: MSE = 0.1975\n",
            "Epoch 84: MSE = 0.1957\n",
            "Epoch 85: MSE = 0.1940\n",
            "Epoch 86: MSE = 0.1923\n",
            "Epoch 87: MSE = 0.1907\n",
            "Epoch 88: MSE = 0.1890\n",
            "Epoch 89: MSE = 0.1874\n",
            "Epoch 90: MSE = 0.1858\n",
            "Epoch 91: MSE = 0.1842\n",
            "Epoch 92: MSE = 0.1827\n",
            "Epoch 93: MSE = 0.1811\n",
            "Epoch 94: MSE = 0.1796\n",
            "Epoch 95: MSE = 0.1781\n",
            "Epoch 96: MSE = 0.1766\n",
            "Epoch 97: MSE = 0.1751\n",
            "Epoch 98: MSE = 0.1737\n",
            "Epoch 99: MSE = 0.1723\n",
            "Epoch 100: MSE = 0.1708\n",
            "Learned weights: [0.7624730188186795, 1.3542960544546545, 1.247291496047205, 0.5604417426266403, 1.484471796775022]\n",
            "Learned bias: 2.853856332628021\n"
          ]
        }
      ]
    }
  ]
}